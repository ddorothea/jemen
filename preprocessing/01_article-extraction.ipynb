{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "\n",
    "faz_dir = data_dir + \"faz/raw-search/\"\n",
    "sz_dir = data_dir + \"sz/raw-search/\"\n",
    "zeit_dir = data_dir + \"zeit/raw-search/\"\n",
    "spiegel_dir = data_dir + \"spiegel/raw-search/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAZ\n",
    "#### Extraktionsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faz_articles(filename):\n",
    "\n",
    "    with open(faz_dir + filename, 'r') as file: \n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "\n",
    "    articles_html = soup.select(\"div[class*='article-row clearfix']\") \n",
    "    articles_list = []\n",
    "\n",
    "    for article in articles_html:\n",
    "        title = article.find(\"h3\").text.strip()\n",
    "        link = \"https://www.faz-biblionet.de/faz-portal/document?uid=\"  + article[\"class\"][2]\n",
    "        date = article.find(\"li\").text.replace(\"|\",\"\")\n",
    "        abstract = article.find(\"p\",{\"class\":\"abstract\"}).text.strip()\n",
    "        h2_list = article.find_all(\"h2\")[:-1]\n",
    "        author = \"\"\n",
    "        surtitle = \"\"\n",
    "        if len(h2_list)>0:\n",
    "            for h2 in h2_list:\n",
    "                if \"Von \" == h2.text.strip()[:4]:\n",
    "                    author = h2.text.strip().replace(\"Von \", \"\").split(\",\")[0]\n",
    "                else:\n",
    "                    surtitle = h2.text.strip()\n",
    "\n",
    "\n",
    "        articles_list.append({\"title\":title, \"link\":link, \"date\":date, \"abstract\":abstract, \"author\":author, \"surtitle\":surtitle, \"source\":\"FAZ\"})\n",
    "    return articles_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ausf端hren der Extraktion (Listenerstellung und Abspeichern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(faz_dir)\n",
    "faz_articles = []\n",
    "\n",
    "for file in files:\n",
    "    faz_articles = faz_articles + extract_faz_articles(file)\n",
    "\n",
    "with open(data_dir + \"faz/all-articles.json\", \"w\") as file:\n",
    "    json.dump(faz_articles, file, indent = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SZ\n",
    "#### Extraktionsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sz_articles(filename):\n",
    "\n",
    "    with open(sz_dir + filename, 'r') as file: \n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "\n",
    "    articles_html = soup.find_all(\"div\", {\"class\":\"hitWrapper\"}) \n",
    "    articles_list = []\n",
    "\n",
    "    for article in articles_html:\n",
    "        title = article.find(\"a\").text.strip()\n",
    "        link = \"https://archiv.szarchiv.de\" + article.find(\"a\")[\"href\"]\n",
    "        date = article.find(\"span\").text.strip()\n",
    "        abstract = article.find(\"div\", {\"class\":\"hitContentFOKUS\"}).text.strip()\n",
    "        author = \"\"\n",
    "        try:\n",
    "            surtitle = article.find(\"div\", {\"class\":\"hitContentDACHZEILE\"}).text.strip()\n",
    "        except:\n",
    "            try: \n",
    "                surtitle = article.find(\"div\", {\"class\":\"hitContentText\"}).text.strip()\n",
    "            except:\n",
    "                surtitle = \"\"\n",
    "       \n",
    "\n",
    "\n",
    "        articles_list.append({\"title\":title, \"link\":link, \"date\":date, \"abstract\":abstract, \"author\":author, \"surtitle\":surtitle, \"source\":\"SZ\"})\n",
    "    return articles_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ausf端hren der Extraktion (Listenerstellung und Abspeichern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(sz_dir)\n",
    "sz_articles = []\n",
    "\n",
    "for file in files:\n",
    "    sz_articles = sz_articles + extract_sz_articles(file)\n",
    "\n",
    "with open(data_dir + \"sz/all-articles.json\", \"w\") as file:\n",
    "    json.dump(sz_articles, file, indent = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zeit\n",
    "#### Wiso-Extraktionsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wiso_articles(filename, directory, source):\n",
    "\n",
    "    with open(directory + filename, 'r') as file: \n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "    articles_html = soup.select(\"tr[class*='hitlist_item']\") \n",
    "    articles_list = []\n",
    "\n",
    "    for article in articles_html:\n",
    "        title = article.select(\"span[class*='boxHeader']\")[0].text.strip()\n",
    "        link = \"https://www.wiso-net.de/\" + article.find(\"a\", {\"rel\":\"nofollow\"})[\"href\"]\n",
    "        date = article.find(\"td\", {\"class\":\"boxCol3\"}).text.strip()\n",
    "        abstract =  article.select(\"span[class*='boxDescription']\")[0].text.strip() + \" \".join([e.text.strip() for e in article.select(\"span[class*='boxAbstract']\")[1:]])\n",
    "        author = \"\"\n",
    "        surtitle = article.select(\"span[class*='boxAbstract']\")[0].text.strip() \n",
    "\n",
    "\n",
    "        articles_list.append({\"title\":title, \"link\":link, \"date\":date, \"abstract\":abstract, \"author\":author, \"surtitle\":surtitle, \"source\":source})\n",
    "    return articles_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ausf端hren der Extraktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(zeit_dir)\n",
    "zeit_articles = []\n",
    "\n",
    "for file in files:\n",
    "    zeit_articles = zeit_articles + extract_wiso_articles(file, zeit_dir, \"Zeit\")\n",
    "\n",
    "with open(data_dir + \"zeit/all-articles.json\", \"w\") as file:\n",
    "    json.dump(zeit_articles, file, indent = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiegel\n",
    "#### f端r surtitle eliminierende Extraktion manipulierte Extraktionsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wiso_articles(filename, directory, source):\n",
    "\n",
    "    with open(directory + filename, 'r') as file: \n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "    articles_html = soup.select(\"tr[class*='hitlist_item']\") \n",
    "    articles_list = []\n",
    "\n",
    "    for article in articles_html:\n",
    "        title = article.select(\"span[class*='boxHeader']\")[0].text.strip()\n",
    "        link = \"https://www.wiso-net.de/\" + article.find(\"a\", {\"rel\":\"nofollow\"})[\"href\"]\n",
    "        date = article.find(\"td\", {\"class\":\"boxCol3\"}).text.strip()\n",
    "        abstract =  article.select(\"span[class*='boxDescription']\")[0].text.strip() + \" \".join([e.text.strip() for e in article.select(\"span[class*='boxAbstract']\")[1:]])\n",
    "        author = \"\"\n",
    "        surtitle = \"\" \n",
    "\n",
    "\n",
    "        articles_list.append({\"title\":title, \"link\":link, \"date\":date, \"abstract\":abstract, \"author\":author, \"surtitle\":surtitle, \"source\":source})\n",
    "    return articles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(spiegel_dir)\n",
    "spiegel_articles = []\n",
    "\n",
    "for file in files:\n",
    "    spiegel_articles = spiegel_articles + extract_wiso_articles(file, spiegel_dir, \"Spiegel\")\n",
    "\n",
    "with open(data_dir + \"spiegel/all-articles.json\", \"w\") as file:\n",
    "    json.dump(spiegel_articles, file, indent = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jemen-scraper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c05e784c37b7486e763435563ea73fc5884b3661493573245e31e915d866c156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
