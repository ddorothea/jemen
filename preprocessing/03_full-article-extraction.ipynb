{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraktion der Artikel aus gescrapten htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "\n",
    "faz_dir = data_dir + \"faz/raw-full-articles/\"\n",
    "zeit_dir = data_dir + \"zeit/raw-full-articles/\"\n",
    "sz_dir = data_dir + \"sz/raw-full-articles/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAZ\n",
    "#### Extraktionsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_faz_article(filename):\n",
    "\n",
    "    with open(faz_dir + filename + \".html\", 'r') as file: \n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "    # ich habe mich an der html zu folgendem Artikel orientiert: FAS__SD1201503294538183\n",
    "    text = soup.find(\"pre\",{\"class\":\"text\"}).text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/faz/all-articles.json\", \"r\") as file:\n",
    "    faz = json.load(file)\n",
    "    faz = [article for article in faz if (\"jemen\" in article[\"title\"].lower()) or (\"jemen\" in article[\"surtitle\"].lower())]\n",
    "\n",
    "for article in faz:\n",
    "    id = article[\"link\"].split(\"uid=\")[1]\n",
    "    text = extract_full_faz_article(id)\n",
    "    article[\"id\"] = id\n",
    "    article[\"text\"]= text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../data/faz/all-relevant-fulltext.json\", \"w\") as file:\n",
    "    json.dump(faz, file, indent = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZEIT\n",
    "#### Extraktionsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_zeit_article(filename):\n",
    "\n",
    "    with open(zeit_dir + filename + \".html\", 'r') as file: \n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "    # ich habe mich an der html zu folgendem Artikel orientiert: view-source:https://www.wiso-net.de//document/ZEIO__5DCBDAE4A902F6F8EE277E8578808207/hitlist/475?all=\n",
    "    text = soup.find(\"pre\",{\"class\":\"text\"}).text.strip()\n",
    "    # Sieht so aus: <pre class=\"boldMedium\">\n",
    "    abstract = soup.find(\"pre\",{\"class\":\"boldMedium\"}).text.strip()\n",
    "    try:\n",
    "        #<pre class=\"italic\">Theo Sommer</pre>\n",
    "        author = soup.find(\"pre\",{\"class\":\"italic\"}).text.strip()\n",
    "    except:\n",
    "        author =\"\"\n",
    "\n",
    "    return text, abstract, author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/zeit/all-articles.json\", \"r\") as file:\n",
    "    zeit = json.load(file)\n",
    "    zeit = [article for article in zeit if (\"jemen\" in article[\"title\"].lower()) or (\"jemen\" in article[\"surtitle\"].lower())]\n",
    "\n",
    "for article in zeit:\n",
    "    id = article[\"link\"].split(\"document/\")[1].split(\"/\")[0]\n",
    "    text, abstract, author = extract_full_zeit_article(id)\n",
    "    # id wird als value mit dem key id in dictionary gespeichert\n",
    "    article[\"id\"] = id\n",
    "    article[\"text\"]= text\n",
    "    article[\"abstract\"]=abstract\n",
    "    article[\"author\"]=author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausführen der Extraktion und Abspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(zeit_dir)\n",
    "zeit_full_articles = []\n",
    "\n",
    "for file in files:\n",
    "    zeit_full_articles = zeit_full_articles + extract_full_zeit_article(file)\n",
    "\n",
    "with open(data_dir + \"zeit/full-articles.json\", \"w\") as file:\n",
    "    json.dump(zeit_full_articles, file, indent = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SZ\n",
    "#### Extraktionsfunktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_sz_articles(filename):\n",
    "\n",
    "    with open(sz_dir + filename, 'r') as file: \n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "#  div[class*]... war das Trennzechen, bei dem ein neuer Artikel begann, brauchen wir das hier?\n",
    "    full_articles_html = soup.select(\"div[class*='article-row clearfix']\") \n",
    "    full_articles_list = []\n",
    "\n",
    "# ich habe mich an der html zu folgendem Artikel orientiert: https://archiv.szarchiv.de/Portal/restricted/Start.act?articleId=A70260928\n",
    "    for article in full_articles_html:\n",
    "        # Sieht so aus: <title>SZ LibraryNet - Volltext Bis zum letzten   Reiskorn (A70260928)</title>\n",
    "        title =  article.find_all(\"title\").text.split(\"Volltext \")[1].split(\" (\")[0].strip\n",
    "        # Es gibt auch die Option, hier einen Perma-Link einzutragen. Sieht so aus: <pre class=\"text permaLink\"><strong>Dauerhafte Adresse des Dokuments: </strong><a href=\"https://www.wiso-net.de/document/ZEIO__932344341ad0602bd3d227a8d80550820d8209ba\">https://www.wiso-net.de/document/ZEIO__932344341ad0602bd3d227a8d80550820d8209ba</a></pre>\n",
    "        link = \"https://archiv.szarchiv.de/Portal/restricted/Start.act?articleId=\"  + str(filename)\n",
    "        # Sieht so aus: \t\n",
    "\t\t\t#<div class=\"fullTextLeftTop\">\n",
    "\t\t\t\t\t#<span>18.03.2017</span>\n",
    "        date = article.find(\"div\",{\"class\":\"fullTextLeftTop\"}).text.strip\n",
    "        # Sieht so aus: <SZ.UT><P><FONT FACE=\"Arial, Helvetica\">Jemens Kriegsparteien nutzen den Mangel als Waffe. <BR />Dringend notwendige Hilfe wird gnadenlos blockiert</FONT></P></SZ.UT>\n",
    "        abstract = ?\n",
    "        # Sieht super strange aus: <P><FONT FACE=\"Arial, Helvetica\"><B>Washington &ndash;</B> F&uuml;r US-Pr&auml;sident Donald Trump geht es um die nationale Sicherheit und um Terrorbek&auml;mpfung. Mit seinem tempor&auml;ren Einreisestopp will er verhindern, dass Menschen aus L&auml;ndern mit &bdquo;terroristischem Hintergrund&ldquo;, in denen die &bdquo;Sicherheits&uuml;berpr&uuml;fung&ldquo; nicht gew&auml;hrleistet sei, in die USA reisen. F&uuml;r Richter Derrick Watson vom Bundesgericht in Hawaii hingegen, der Trumps Einreisestopp am Mittwochabend vorl&auml;ufig gestoppt hat, geht es um die Verfassung der Vereinigten Staaten. Der Einreisestopp, so Watson, diskriminiere eine ganze Religion und sei deshalb verfassungswidrig. </FONT></P>\n",
    "            #<P><FONT FACE=\"Arial, Helvetica\">Es ist das zweite Mal,\n",
    "        text = article.find(\"pre\",{\"class\":\"text\"}).text.strip()\n",
    "        # Steht manchmal oben: <P ALIGN=\"CENTER\"><FONT FACE=\"Arial, Helvetica\"><AUTHOR><B>VON <AUTHORNAME>PAUL-ANTON KR&Uuml;GER</AUTHORNAME></B></AUTHOR></FONT></P>\n",
    "        # Und manchmal unten als Kürzel: <P><FONT FACE=\"sans-serif\" SIZE=\"-1\"><AUTHOR>SZ.de/rtr</AUTHOR></FONT></P>\n",
    "        # Und manchmal unten rechts: <P ALIGN=\"RIGHT\"><FONT FACE=\"Arial, Helvetica\"><AUTHOR><B><AUTHORNAME>SACHA BATTHYANY</AUTHORNAME> </B></AUTHOR></FONT></P>\n",
    "        # Und manchmal am Anfang in kursiv: <author>Analyse von <authorname>Kathleen Hildebrand</authorname></author>\n",
    "        author = article.find(\"author\").text.split(\"de/\")[1].strip\n",
    "        # wenn es einen gibt: <SZ.DT><P><FONT FACE=\"sans-serif\"><B>Migration</B></FONT></P></SZ.DT> \n",
    "        surtitle = article.find_all(\"pre\",{\"class\":\"text\"}).text\n",
    "\n",
    "\n",
    "        full_articles_list.append({\"title\":title, \"link\":link, \"date\":date, \"abstract\":abstract, \"author\":author, \"surtitle\":surtitle, \"source\":\"ZEIT\", \"text\": text})\n",
    "    return full_articles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausführen der Extraktion und Abspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(sz_dir)\n",
    "sz_full_articles = []\n",
    "\n",
    "for file in files:\n",
    "    sz_full_articles = sz_full_articles + extract_full_sz_articles(file)\n",
    "\n",
    "with open(data_dir + \"sz/full-articles.json\", \"w\") as file:\n",
    "    json.dump(sz_full_articles, file, indent = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('jemen-scraper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c05e784c37b7486e763435563ea73fc5884b3661493573245e31e915d866c156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
